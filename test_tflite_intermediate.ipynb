{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0c1abdd2cb8e2bb7bc61f0f837085c52fe48b926da48959e95678065ce71dcf54",
   "display_name": "Python 3.7.10 64-bit ('tf1.13': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import input_data\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats(train_data, val_data, test_data):\n",
    "    \"\"\"mean and std_dev\n",
    "\n",
    "        Args:\n",
    "            train_data: (36923, 490)\n",
    "            val_data: (4445, 490)\n",
    "            test_data: (4890, 490)\n",
    "\n",
    "        Return: (mean, std_dev)\n",
    "\n",
    "        Result:\n",
    "            mean: -3.975149608704592, 220.81257374779565\n",
    "            std_dev: 0.8934739293234528\n",
    "    \"\"\"\n",
    "    print(train_data.shape, val_data.shape, test_data.shape)\n",
    "    all_data = np.concatenate((train_data, val_data, test_data), axis=0)\n",
    "    std_dev = 255. / (all_data.max() - all_data.min())\n",
    "    # mean_ = all_data.mean()\n",
    "    mean_ = 255. * all_data.min() / (all_data.min() - all_data.max())\n",
    "    return (mean_, std_dev)\n",
    "\n",
    "def fp32_to_uint8(r):\n",
    "    # method 1\n",
    "    # s = (r.max() - r.min()) / 255.\n",
    "    # z = 255. - r.max() / s\n",
    "    # q = r / s + z\n",
    "\n",
    "    # method 2\n",
    "    std_dev = 0.8934739293234528\n",
    "    mean_ = 220.81257374779565\n",
    "    q = r / std_dev + mean_\n",
    "    q = q.astype(np.uint8)\n",
    "    return q\n",
    "\n",
    "def calc(interpreter, input_data, label):\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # print(input_details)\n",
    "    # print(output_details)\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # print(output_data)\n",
    "    # print(label)\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = 'yes,no,up,down,left,right,on,off,stop,go'\n",
    "sample_rate = 16000\n",
    "clip_duration_ms = 1000\n",
    "model_architecture = 'mobilenet-v3'\n",
    "dct_coefficient_count = 10\n",
    "batch_size = 1\n",
    "window_size_ms = 40\n",
    "window_stride_ms = 20\n",
    "model_size_info = [4, 16, 10, 4, 2, 2, 16, 3, 3, 1, 1, 2, 32, 3, 3, 1, 1, 2, 32, 5, 5, 1, 1, 2]\n",
    "data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "data_dir = '/tmp/speech_dataset/'\n",
    "silence_percentage = 10.0\n",
    "unknown_percentage = 10.0\n",
    "testing_percentage = 10\n",
    "validation_percentage = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of layers: %d\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/utilities.py:144: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/nas_model.py:151: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/nas_model.py:156: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess = tf.InteractiveSession()\n",
    "words_list = input_data.prepare_words_list(wanted_words.split(','))\n",
    "model_settings = models.prepare_model_settings(\n",
    "    len(words_list), sample_rate, clip_duration_ms, window_size_ms,\n",
    "    window_stride_ms, dct_coefficient_count)\n",
    "\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    data_url, data_dir, silence_percentage,\n",
    "    unknown_percentage,\n",
    "    wanted_words.split(','), validation_percentage,\n",
    "    testing_percentage, model_settings)\n",
    "\n",
    "label_count = model_settings['label_count']\n",
    "fingerprint_size = model_settings['fingerprint_size']\n",
    "\n",
    "fingerprint_input = tf.placeholder(\n",
    "    tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "\n",
    "logits = models.create_model(\n",
    "    fingerprint_input,\n",
    "    model_settings,\n",
    "    model_architecture,\n",
    "    model_size_info,\n",
    "    is_training=False)\n",
    "\n",
    "ground_truth_input = tf.placeholder(\n",
    "    tf.float32, [None, label_count], name='groundtruth_input')\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "expected_indices = tf.argmax(ground_truth_input, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, expected_indices)\n",
    "confusion_matrix = tf.confusion_matrix(\n",
    "    expected_indices, predicted_indices, num_classes=label_count)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "class Transformer_bd(object):\n",
    "    def __init__(self, precision):\n",
    "        self.pre = precision\n",
    "        self.shift_num = None\n",
    "\n",
    "    def bTod(self, n):\n",
    "        '''\n",
    "        把一个带小数的二进制数n转换成十进制\n",
    "        小数点后面保留pre位小数\n",
    "        '''\n",
    "        string_number1 = str(n) #number1 表示二进制数，number2表示十进制数\n",
    "        decimal = 0  #小数部分化成二进制后的值\n",
    "        flag = False   \n",
    "        for i in string_number1: #判断是否含小数部分\n",
    "            if i == '.':\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: #若二进制数含有小数部分\n",
    "            string_integer, string_decimal = string_number1.split('.') #分离整数部分和小数部分\n",
    "            for i in range(len(string_decimal)):\n",
    "                decimal += 2**(-i-1)*int(string_decimal[i])  #小数部分化成二进制\n",
    "            number2 = int(str(int(string_integer, 2))) + decimal\n",
    "            return round(number2, self.pre)\n",
    "        else: #若二进制数只有整数部分\n",
    "            return int(string_number1, 2)#若只有整数部分 直接一行代码二进制转十进制 python还是骚 \n",
    "    \n",
    "    def dTob(self, n, shift=False):\n",
    "        '''\n",
    "        把十进制的浮点数n转换成二进制\n",
    "        小数点后面保留pre位小数\n",
    "        '''\n",
    "        string_number1 = str(n) #number1 表示十进制数，number2表示二进制数\n",
    "        flag = False   \n",
    "        for i in string_number1: #判断是否含小数部分\n",
    "            if i == '.':\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            string_integer, string_decimal = string_number1.split('.') #分离整数部分和小数部分\n",
    "            integer = int(string_integer)\n",
    "            decimal = Decimal(str(n)) - integer\n",
    "            l1 = [0,1]\n",
    "            l2 = []\n",
    "            decimal_convert = \"\"\n",
    "            while True:  \n",
    "                if integer == 0: break\n",
    "                x,y = divmod(integer, 2)  #x为商，y为余数 \n",
    "                l2.append(y)\n",
    "                integer = x\n",
    "            string_integer = ''.join([str(j) for j in l2[::-1]])  #整数部分转换成二进制 \n",
    "            i = 0  \n",
    "            while decimal != 0 and i < self.pre:  \n",
    "                result = int(decimal * 2)  \n",
    "                decimal = decimal * 2 - result  \n",
    "                decimal_convert = decimal_convert + str(result)  \n",
    "                i = i + 1  \n",
    "            string_number2 = string_integer + '.' + decimal_convert\n",
    "            # return float(string_number2)\n",
    "\n",
    "            # 这里进行移位修改\n",
    "            if string_integer == '':\n",
    "                string_integer = '0'\n",
    "\n",
    "            if shift == True:\n",
    "                string_number3 = decimal_convert.lstrip('0')\n",
    "                lshift_num = len(decimal_convert) - len(string_number3) # 左移的位数\n",
    "                self.shift_num = lshift_num\n",
    "                return string_integer + '.' + string_number3\n",
    "            else:\n",
    "                return string_integer + '.' + decimal_convert\n",
    "\n",
    "        else: #若十进制只有整数部分\n",
    "            l1 = [0,1]\n",
    "            l2 = []\n",
    "            while True:  \n",
    "                if n == 0: break\n",
    "                x,y = divmod(n, 2)  #x为商，y为余数 \n",
    "                l2.append(y)\n",
    "                n = x\n",
    "            string_number = ''.join([str(j) for j in l2[::-1]])  \n",
    "            # return int(string_number)\n",
    "            return string_number\n",
    "        \n",
    "\n",
    "    def right_shift(self, s):\n",
    "        '''右移二进制字符串'''\n",
    "        point_pos = s.find('.') # point_pos的值就说明小数点前面有多少个数字\n",
    "        ss = s[:point_pos] + s[point_pos + 1:]\n",
    "        if self.shift_num > point_pos:\n",
    "            zero_fill = self.shift_num - point_pos   # 需要补0的个数\n",
    "            for _ in range(zero_fill):\n",
    "                ss = '0' + ss\n",
    "            return '0.' + ss\n",
    "        else:\n",
    "            insert_pos = point_pos - self.shift_num\n",
    "            ssl = list(ss)\n",
    "            ssl.insert(insert_pos, '.')\n",
    "            ss = ''.join(ssl)\n",
    "            if insert_pos == 0:\n",
    "                ss = '0' + ss   # 在头部补零\n",
    "            return ss\n",
    "\n",
    "    def lshift_decimal(self, num):\n",
    "        '''得到左移后的十进制'''\n",
    "        num = self.dTob(num, shift=True)\n",
    "        num = self.bTod(num)\n",
    "        return num\n",
    "\n",
    "    def rshift_decimal(self, num):\n",
    "        '''得到右移后的十进制'''\n",
    "        num = self.dTob(num, shift=False)\n",
    "        num = self.right_shift(num)\n",
    "        num = self.bTod(num)\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a * b = 0.15\n0.5119999647140503\nbefore right shift, a * num2 = 76.79999470710754\n\n0.0009999999310821295\nafter right shift, a * b = 0.14999998966231942\n"
     ]
    }
   ],
   "source": [
    "a = 150\n",
    "num = 0.001\n",
    "print('a * b =', a*num)\n",
    "\n",
    "b = Transformer_bd(32)\n",
    "num2 = b.lshift_decimal(num)\n",
    "print(num2)\n",
    "print('before right shift, a * num2 =', a*num2)\n",
    "print()\n",
    "\n",
    "aft_shift = b.rshift_decimal(num2)\n",
    "print(aft_shift)\n",
    "num_aft = a * aft_shift\n",
    "print('after right shift, a * b =', num_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/stem_conv.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_projection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_add.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_projection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_projection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_add.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/AvgPool.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/Conv2D.lite'\n",
    "\n",
    "# Boss\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/uint8input_8bit_calc_mean220_std0.89.lite'\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"tflite_factory/swiftnet-uint8.lite\")\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_int(input_data):\n",
    "    ################## stem conv ##################\n",
    "    print('stem conv')\n",
    "    new_data = input_data.reshape(-1, 49, 10, 1)\n",
    "    new_data = tf.convert_to_tensor(new_data, tf.float32, name='input')\n",
    "    new_data =  new_data - 221.\n",
    "    s_iwr = tf.constant(0.0011018913937732577 / 0.16148914396762848, tf.float32)\n",
    "    s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32, name='weight')\n",
    "    weight = weight - 132.\n",
    "    weight = tf.transpose(weight, perm=[1,2,0,3])\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32, name='bias')\n",
    "    print(weight)\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,2,2,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None,  # 数据格式，与步长参数配合，决定移动方式\n",
    "                name='stem_conv') # 名字，用于tensorboard图形显示时使用\n",
    "\n",
    "    output = tf.add(output, bias, name='add')\n",
    "    output *= s_iwr\n",
    "    output += 0.0035\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output, name='round')\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8, name='uint8')\n",
    "    add_2 = tf.identity(output_uint8)   # 给之后的做加法\n",
    "    print()\n",
    "\n",
    "    # ################## inverted residual 1 expansion ##################\n",
    "    # print('inverted residual 1 expansion')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.003049441846087575 / 0.27361148595809937, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 146\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.0074\n",
    "    # output *= s_iwr\n",
    "    \n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 1 depthwise ##################\n",
    "    # print('inverted residual 1 depthwise')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.0045695919543504715 / 0.12676289677619934, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 127\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.0301\n",
    "    # output *= s_iwr\n",
    "    \n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 1 projection ##################\n",
    "    # print('inverted residual 1 projection')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.0009397256653755903 / 0.16901935636997223, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 101\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.00052\n",
    "    # output = output * s_iwr + 133\n",
    "    \n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # add_1 = tf.identity(output_uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 1 add ##################\n",
    "    # add_1 = tf.cast(add_1, tf.float32)\n",
    "    # add_2 = tf.cast(add_2, tf.float32)\n",
    "\n",
    "    # add_1 = tf.constant(0.16901935636997223, tf.float32) * (add_1 - 133)\n",
    "    # add_2 = tf.constant(0.16148914396762848, tf.float32) * add_2\n",
    "\n",
    "    # output_result = tf.add(add_1, add_2)\n",
    "    # output = output_result / tf.constant(0.24699252843856812, tf.float32) + 89\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    # ################## inverted residual 2 expansion ##################\n",
    "    # print('inverted residual 2 expansion')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # new_data -= 89\n",
    "    # s_iwr = tf.constant(0.0020657109562307596 / 0.09814818948507309, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 149\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.01062\n",
    "    # output *= s_iwr\n",
    "\n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 2 depthwise ##################\n",
    "    # print('inverted residual 2 depthwise')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.0014443636173382401 / 0.062810979783535, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 120\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.0153\n",
    "    # output *= s_iwr\n",
    "    \n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 2 projection ##################\n",
    "    # print('inverted residual 2 projection')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.00040918667218647897 / 0.0929793044924736, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 148\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # output = output * s_iwr + 138\n",
    "    \n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # add_2 = tf.identity(output_uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 3 expansion ##################\n",
    "    # print('inverted residual 3 expansion')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # new_data -= 138\n",
    "    # s_iwr = tf.constant(0.0005567758926190436 / 0.07842949777841568, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 137\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # # output += 0.00113\n",
    "    # output *= s_iwr\n",
    "    \n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 3 depthwise ##################\n",
    "    # print('inverted residual 3 depthwise')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.013642110861837864 / 0.05131378769874573, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 79\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # output *= s_iwr\n",
    "    \n",
    "    # output = tf.nn.relu(output)\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 3 projection ##################\n",
    "    # print('inverted residual 3 projection')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # s_iwr = tf.constant(0.0008600406581535935 / 0.20826007425785065, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 125\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # output = output * s_iwr + 133\n",
    "    \n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # add_1 = tf.identity(output_uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## inverted residual 3 add ##################\n",
    "    # add_1 = tf.cast(add_1, tf.float32)\n",
    "    # add_2 = tf.cast(add_2, tf.float32)\n",
    "\n",
    "    # add_1 = tf.constant(0.20826007425785065, tf.float32) * (add_1 - 133)\n",
    "    # add_2 = tf.constant(0.0929793044924736, tf.float32) * (add_2 - 138)\n",
    "\n",
    "    # output_result = tf.add(add_1, add_2)\n",
    "    # output_uint8 = output_result / tf.constant(0.21021947264671326, tf.float32) + 131\n",
    "    # output_uint8 = tf.math.round(output_uint8)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    # ################## AvgPool ##################\n",
    "    # # method 1\n",
    "    # # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # # new_data = 0.21021947264671326 * (new_data - 131)\n",
    "    # # output = tf.nn.avg_pool(new_data,\n",
    "    # #                 ksize=[1,25,5,1],\n",
    "    # #                 strides=[1,25,5,1],\n",
    "    # #                 padding='VALID')\n",
    "    # # output = output / 0.21021947264671326 + 131\n",
    "    # # output_uint8 = tf.math.round(output)\n",
    "    # # output_uint8 = tf.cast(output, tf.uint8)\n",
    "\n",
    "    # # method 2 (简化版本，发现scale和zero_point完全可以消除)\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # output = tf.nn.avg_pool(new_data,\n",
    "    #                 ksize=[1,25,5,1],\n",
    "    #                 strides=[1,25,5,1],\n",
    "    #                 padding='VALID')\n",
    "    # # output -= 0.0041\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    # ################## Conv2D ##################\n",
    "    # print('Conv2D')\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # new_data -= 131\n",
    "    # s_iwr = tf.constant(0.0033858332317322493 / 0.1784215271472931, tf.float16)\n",
    "    # s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    # weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_fc_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    # bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_fc_conv_Conv2D_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    # weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    # weight -= 143\n",
    "    # weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    # print(weight)\n",
    "\n",
    "    # bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    # print(bias)\n",
    "\n",
    "    # output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "    #             filter=weight, # 卷积核参数\n",
    "    #             strides=[1,1,1,1], # 步长参数\n",
    "    #             padding=\"SAME\", # 卷积方式\n",
    "    #             data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    # output = output + bias\n",
    "    # output = output * s_iwr + 129\n",
    "    \n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    # add_1 = tf.identity(output_uint8)\n",
    "    # print()\n",
    "\n",
    "    # ################## Reshape ##################\n",
    "    # output_uint8 = tf.squeeze(output_uint8, axis=[1,2])\n",
    "\n",
    "    # ################## Softmax ##################\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # new_data = tf.constant(0.1784215271472931, tf.float32) * (new_data - 129)\n",
    "    # output = tf.nn.softmax(new_data)\n",
    "    # output = output / tf.constant(0.00390625, tf.float32)\n",
    "\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    ################## running ##################\n",
    "\n",
    "    return output_uint8.eval(), output.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(output_uint8):\n",
    "    add_2 = np.load('test_log/mobilenetv3_quant_eval/debug/add_2.npy')\n",
    "    ################## inverted residual 1 add ##################\n",
    "    add_1 = tf.cast(output_uint8, tf.float32)\n",
    "    add_2 = tf.convert_to_tensor(add_2, tf.float32)\n",
    "    add_1_scale = tf.constant(0.16901935636997223, tf.float32)\n",
    "    add_2_scale = tf.constant(0.16148914396762848, tf.float32)\n",
    "\n",
    "    add_1 = add_1_scale * (add_1 - 133)\n",
    "    add_2 = add_2_scale * add_2\n",
    "\n",
    "    # low = tf.ones_like(add_1) * -22.47957420349121\n",
    "    # high = tf.ones_like(add_1) * 20.620361328125\n",
    "    # add_1 = tf.where(add_1 < -22.47957420349121, low, add_1)\n",
    "    # add_1 = tf.where(add_1 > 20.620361328125, high, add_1)\n",
    "\n",
    "    # high = tf.ones_like(add_2) * 20.620361328125\n",
    "    # add_2 = tf.where(add_2 > 20.620361328125, high, add_2)\n",
    "\n",
    "    output_result = tf.math.add(add_1, add_2)\n",
    "    output_scale = tf.constant(0.24699252843856812, tf.float32)\n",
    "    output = output_result / output_scale + 89\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.math.floor(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    return output_uint8.eval(), output.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_real(input_data):\n",
    "    '''\n",
    "    直接用float32复现\n",
    "    '''\n",
    "    ################## stem conv ##################\n",
    "    print('stem conv')\n",
    "    precision = 32\n",
    "    new_data = input_data.reshape(-1, 49, 10, 1)\n",
    "    new_data = tf.convert_to_tensor(new_data, tf.float32, name='input')\n",
    "    new_data =  new_data - 221.\n",
    "    # s_iw = tf.constant(1.1192268133163452, tf.float32) * tf.constant(0.0009845112217590213, tf.float32)\n",
    "    s_iwr = tf.constant(0.0011018913937732577 / 0.16148914396762848, tf.float16)\n",
    "    s_iwr = tf.cast(s_iwr, tf.float32)\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32, name='weight')\n",
    "    weight = weight - 132.\n",
    "    weight = tf.transpose(weight, perm=[1,2,0,3])\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32, name='bias')\n",
    "    print(weight)\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,2,2,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None,  # 数据格式，与步长参数配合，决定移动方式\n",
    "                name='stem_conv') # 名字，用于tensorboard图形显示时使用\n",
    "    output_fp = output\n",
    "\n",
    "    output_add = tf.add(output_fp, bias, name='add')\n",
    "    output_add = output_add * s_iwr\n",
    "    \n",
    "    output = tf.nn.relu(output_add)\n",
    "    output_uint8 = tf.math.round(output, name='round')\n",
    "    # output_uint8 = tf.math.floor(output, name='round')\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8, name='uint8')\n",
    "    add_2 = tf.identity(output_uint8)   # 给之后的做加法\n",
    "    # np.save('test_log/mobilenetv3_quant_eval/debug/add_2.npy', add_2.eval())\n",
    "    print()\n",
    "\n",
    "    # output_uint8 = sess.run(output_uint8)\n",
    "    return output_uint8.eval(), output.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:set_size=4890\n",
      "stem conv\n",
      "uint8 (1, 10, 4, 16)\n",
      "int32 (16,)\n",
      "Tensor(\"transpose_1582:0\", shape=(10, 4, 1, 16), dtype=float32)\n",
      "Tensor(\"bias_557:0\", shape=(16,), dtype=float32)\n",
      "\n",
      "[ 0.49810204  5.4995923   4.496565    4.496565    7.498824    2.4973335\n",
      "  5.4995923  11.497287    0.49810204  2.4973335  18.498009  ]\n",
      "[ 1  6  5  5  8  3  6 12  1  3 19]\n",
      "add [0.5004077, 0.5004077, 0.5011759, 0.50189793, 0.50189793, 0.5019913, 0.5026665, 0.5026665, 0.5027132, 0.50343513, 0.50343513]\n",
      "11 / 2000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "0",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "set_size = audio_processor.set_size('testing')\n",
    "tf.logging.info('set_size=%d', set_size)\n",
    "total_accuracy = 0\n",
    "for i in range(0, set_size, batch_size):\n",
    "    test_fingerprints, test_ground_truth = audio_processor.get_data(\n",
    "        batch_size, i, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "    # print(test_fingerprints.shape)  # (batch_size 490)\n",
    "    # print(test_ground_truth.shape)  # (batch_size, 12)\n",
    "    test_fingerprints = fp32_to_uint8(test_fingerprints)\n",
    "    output_simulate, output_ = manual_int(test_fingerprints)\n",
    "    # output_simulate, output_ = manual_real(test_fingerprints)\n",
    "    output_real = calc(interpreter, test_fingerprints, test_ground_truth)\n",
    "    # output_simulate, output_ = debug(output_real)\n",
    "\n",
    "    # real_output\n",
    "    # np.save('test_log/mobilenetv3_quant_eval/debug/add_2.npy', output_real)\n",
    "    # np.save('test_log/mobilenetv3_quant_eval/debug/output_real.npy', output_real)\n",
    "    # output_real = np.load('test_log/mobilenetv3_quant_eval/debug/output_real.npy')\n",
    "\n",
    "    # print(output_simulate.shape)\n",
    "    # print(output_real.shape)\n",
    "    # print(output_simulate)\n",
    "    # print(output_real)\n",
    "    # print(test_ground_truth)\n",
    "\n",
    "    # sys.exit(0)\n",
    "    neq = output_simulate != output_real\n",
    "    print(output_[neq])\n",
    "    print(output_real[neq])\n",
    "    print('add', sorted(output_real[neq] - output_[neq]))\n",
    "    # print('sub', sorted(output_[neq] - output_real[neq]))\n",
    "    \n",
    "    eq = tf.equal(output_real, output_simulate)\n",
    "    mask = tf.cast(tf.zeros_like(eq), tf.bool)\n",
    "    neq = tf.reduce_sum(tf.cast(tf.equal(eq, mask), tf.int32))\n",
    "    print(sess.run(neq), '/', sess.run(tf.size(eq)))\n",
    "\n",
    "    sys.exit(0)\n",
    "    # batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "\n",
    "\n",
    "'''\n",
    "############################### get all data mean and std_dev ###############################\n",
    "training_fingerprints, training_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'training', sess)\n",
    "validation_fingerprints, validation_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'validation', sess)\n",
    "testing_fingerprints, testing_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "mean_, std_dev = data_stats(training_fingerprints, validation_fingerprints, testing_fingerprints)\n",
    "print(mean_, std_dev)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}