{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0c1abdd2cb8e2bb7bc61f0f837085c52fe48b926da48959e95678065ce71dcf54",
   "display_name": "Python 3.7.10 64-bit ('tf1.13': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import input_data\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats(train_data, val_data, test_data):\n",
    "    \"\"\"mean and std_dev\n",
    "\n",
    "        Args:\n",
    "            train_data: (36923, 490)\n",
    "            val_data: (4445, 490)\n",
    "            test_data: (4890, 490)\n",
    "\n",
    "        Return: (mean, std_dev)\n",
    "\n",
    "        Result:\n",
    "            mean: -3.975149608704592, 220.81257374779565\n",
    "            std_dev: 0.8934739293234528\n",
    "    \"\"\"\n",
    "    print(train_data.shape, val_data.shape, test_data.shape)\n",
    "    all_data = np.concatenate((train_data, val_data, test_data), axis=0)\n",
    "    std_dev = 255. / (all_data.max() - all_data.min())\n",
    "    # mean_ = all_data.mean()\n",
    "    mean_ = 255. * all_data.min() / (all_data.min() - all_data.max())\n",
    "    return (mean_, std_dev)\n",
    "\n",
    "def fp32_to_uint8(r):\n",
    "    # method 1\n",
    "    # s = (r.max() - r.min()) / 255.\n",
    "    # z = 255. - r.max() / s\n",
    "    # q = r / s + z\n",
    "\n",
    "    # method 2\n",
    "    std_dev = 0.8934739293234528\n",
    "    mean_ = 220.81257374779565\n",
    "    q = r / std_dev + mean_\n",
    "    q = q.astype(np.uint8)\n",
    "    return q\n",
    "\n",
    "def calc(interpreter, input_data, label):\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # print(input_details)\n",
    "    # print(output_details)\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # print(output_data)\n",
    "    # print(label)\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_words = 'yes,no,up,down,left,right,on,off,stop,go'\n",
    "sample_rate = 16000\n",
    "clip_duration_ms = 1000\n",
    "model_architecture = 'mobilenet-v3'\n",
    "dct_coefficient_count = 10\n",
    "batch_size = 1\n",
    "window_size_ms = 40\n",
    "window_stride_ms = 20\n",
    "model_size_info = [4, 16, 10, 4, 2, 2, 16, 3, 3, 1, 1, 2, 32, 3, 3, 1, 1, 2, 32, 5, 5, 1, 1, 2]\n",
    "data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "data_dir = '/tmp/speech_dataset/'\n",
    "silence_percentage = 10.0\n",
    "unknown_percentage = 10.0\n",
    "testing_percentage = 10\n",
    "validation_percentage = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of layers: %d\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/utilities.py:144: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/nas_model.py:151: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From /share/Documents/project/nnx-kws-ne001/nas_model.py:156: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/tf1.13/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess = tf.InteractiveSession()\n",
    "words_list = input_data.prepare_words_list(wanted_words.split(','))\n",
    "model_settings = models.prepare_model_settings(\n",
    "    len(words_list), sample_rate, clip_duration_ms, window_size_ms,\n",
    "    window_stride_ms, dct_coefficient_count)\n",
    "\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    data_url, data_dir, silence_percentage,\n",
    "    unknown_percentage,\n",
    "    wanted_words.split(','), validation_percentage,\n",
    "    testing_percentage, model_settings)\n",
    "\n",
    "label_count = model_settings['label_count']\n",
    "fingerprint_size = model_settings['fingerprint_size']\n",
    "\n",
    "fingerprint_input = tf.placeholder(\n",
    "    tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "\n",
    "logits = models.create_model(\n",
    "    fingerprint_input,\n",
    "    model_settings,\n",
    "    model_architecture,\n",
    "    model_size_info,\n",
    "    is_training=False)\n",
    "\n",
    "ground_truth_input = tf.placeholder(\n",
    "    tf.float32, [None, label_count], name='groundtruth_input')\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "expected_indices = tf.argmax(ground_truth_input, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, expected_indices)\n",
    "confusion_matrix = tf.confusion_matrix(\n",
    "    expected_indices, predicted_indices, num_classes=label_count)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/stem_conv.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_1_projection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_2_protection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_expansion.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_depthwise.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_projection.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/inverted_residual_3_add.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/AvgPool.lite'\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/layers_lite_model/Conv2D.lite'\n",
    "\n",
    "# Boss\n",
    "# tflite_path = 'test_log/mobilenetv3_quant_eval/uint8input_8bit_calc_mean220_std0.89.lite'\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"tflite_factory/swiftnet-uint8.lite\")\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_int(input_data):\n",
    "    ################## stem conv ##################\n",
    "    print('stem conv')\n",
    "    new_data = input_data.reshape(-1, 49, 10, 1)\n",
    "    new_data = tf.convert_to_tensor(new_data, tf.float32, name='input')\n",
    "    new_data =  new_data - 221.\n",
    "    s_iw = 1.1192268133163452 * 0.0009845112217590213\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32, name='weight')\n",
    "    weight = weight - 132.\n",
    "    weight = tf.transpose(weight, perm=[1,2,0,3])\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32, name='bias')\n",
    "    bias = 0.0011018913937732577 * bias\n",
    "    print(weight)\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,2,2,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None,  # 数据格式，与步长参数配合，决定移动方式\n",
    "                name='stem_conv') # 名字，用于tensorboard图形显示时使用\n",
    "    output_fp = s_iw * output\n",
    "\n",
    "    output_add = tf.add(output_fp, bias, name='add')\n",
    "    # output_uint8 = tf.cast(output, dtype=tf.uint8)\n",
    "    output_add = output_add / 0.16148914396762848\n",
    "    \n",
    "    output_relu = tf.nn.relu(output_add)\n",
    "    output_uint8 = tf.math.round(output_relu, name='round')\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8, name='uint8')\n",
    "    add_2 = tf.identity(output_uint8)   # 给之后的做加法\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 1 expansion ##################\n",
    "    print('inverted residual 1 expansion')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.16148914396762848 * 0.01888326182961464\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 146\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.003049441846087575\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.27361148595809937\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 1 depthwise ##################\n",
    "    print('inverted residual 1 depthwise')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.27361148595809937 * 0.016701024025678635\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 127\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0045695919543504715\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.12676289677619934\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 1 projection ##################\n",
    "    print('inverted residual 1 projection')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.12676289677619934 * 0.007413254585117102\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_1_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 101\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0009397256653755903\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.16901935636997223 + 133\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    add_1 = tf.identity(output_uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 1 add ##################\n",
    "    add_1 = tf.cast(add_1, tf.float32)\n",
    "    add_2 = tf.cast(add_2, tf.float32)\n",
    "\n",
    "    add_1 = 0.16901935636997223 * (add_1 - 133)\n",
    "    add_2 = 0.16148914396762848 * add_2\n",
    "\n",
    "    output_result = tf.add(add_1, add_2)\n",
    "    output_uint8 = output_result / 0.24699252843856812 + 89\n",
    "    output_uint8 = tf.math.round(output_uint8)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    ################## inverted residual 2 expansion ##################\n",
    "    print('inverted residual 2 expansion')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    new_data -= 89\n",
    "    s_iw = 0.24699252843856812 * 0.008363455533981323\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 149\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0020657109562307596\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.09814818948507309\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 2 depthwise ##################\n",
    "    print('inverted residual 2 depthwise')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.09814818948507309 * 0.014716151170432568\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 120\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0014443636173382401\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.062810979783535\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 2 projection ##################\n",
    "    print('inverted residual 2 projection')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.062810979783535 * 0.006514572538435459\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_2_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 148\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.00040918667218647897\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.0929793044924736 + 138\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    add_2 = tf.identity(output_uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 3 expansion ##################\n",
    "    print('inverted residual 3 expansion')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    new_data -= 138\n",
    "    s_iw = 0.0929793044924736 * 0.005988169927150011\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_expansion_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_expansion_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 137\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0005567758926190436\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.07842949777841568\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 3 depthwise ##################\n",
    "    print('inverted residual 3 depthwise')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.07842949777841568 * 0.17394107580184937\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_depthwise_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_depthwise_depthwise_conv_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 79\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.013642110861837864\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.05131378769874573\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 3 projection ##################\n",
    "    print('inverted residual 3 projection')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    s_iw = 0.05131378769874573 * 0.01676042005419731\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_projection_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_inverted_residual_3_projection_conv_Conv2D_Fold_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 125\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0008600406581535935\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.20826007425785065 + 133\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    add_1 = tf.identity(output_uint8)\n",
    "    print()\n",
    "\n",
    "    ################## inverted residual 1 add ##################\n",
    "    add_1 = tf.cast(add_1, tf.float32)\n",
    "    add_2 = tf.cast(add_2, tf.float32)\n",
    "\n",
    "    add_1 = 0.20826007425785065 * (add_1 - 133)\n",
    "    add_2 = 0.0929793044924736 * (add_2 - 138)\n",
    "\n",
    "    output_result = tf.add(add_1, add_2)\n",
    "    output_uint8 = output_result / 0.21021947264671326 + 131\n",
    "    output_uint8 = tf.math.round(output_uint8)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    ################## AvgPool ##################\n",
    "    # method 1\n",
    "    # new_data = tf.cast(output_uint8, tf.float32)\n",
    "    # new_data = 0.21021947264671326 * (new_data - 131)\n",
    "    # output = tf.nn.avg_pool(new_data,\n",
    "    #                 ksize=[1,25,5,1],\n",
    "    #                 strides=[1,25,5,1],\n",
    "    #                 padding='VALID')\n",
    "    # output = output / 0.21021947264671326 + 131\n",
    "    # output_uint8 = tf.math.round(output)\n",
    "    # output_uint8 = tf.cast(output, tf.uint8)\n",
    "\n",
    "    # method 2 (简化版本，发现scale和zero_point完全可以消除)\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    output = tf.nn.avg_pool(new_data,\n",
    "                    ksize=[1,25,5,1],\n",
    "                    strides=[1,25,5,1],\n",
    "                    padding='VALID')\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output, tf.uint8)\n",
    "\n",
    "    ################## Conv2D ##################\n",
    "    print('Conv2D')\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    new_data -= 131\n",
    "    s_iw = 0.21021947264671326 * 0.01610618270933628\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_fc_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_fc_conv_Conv2D_bias.npy')\n",
    "    print(weight.dtype, weight.shape)\n",
    "    print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32)\n",
    "    weight -= 143\n",
    "    weight = tf.transpose(weight, perm=[1,2,3,0])\n",
    "    print(weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32)\n",
    "    bias *= 0.0033858332317322493\n",
    "    print(bias)\n",
    "\n",
    "    output = tf.nn.conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,1,1,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None)  # 数据格式，与步长参数配合，决定移动方式\n",
    "    output = s_iw * output\n",
    "    output = output + bias\n",
    "    output = output / 0.1784215271472931 + 129\n",
    "    \n",
    "    output = tf.nn.relu(output)\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "    add_1 = tf.identity(output_uint8)\n",
    "    print()\n",
    "\n",
    "    ################## Reshape ##################\n",
    "    output_uint8 = tf.squeeze(output_uint8, axis=[1,2])\n",
    "\n",
    "    ################## Softmax ##################\n",
    "    new_data = tf.cast(output_uint8, tf.float32)\n",
    "    new_data = 0.1784215271472931 * (new_data - 129)\n",
    "    output = tf.nn.softmax(new_data)\n",
    "    output = output / 0.00390625\n",
    "\n",
    "    output_uint8 = tf.math.round(output)\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8)\n",
    "\n",
    "    ################## running ##################\n",
    "\n",
    "    # print(input_data)\n",
    "    with tf.Session() as sess:\n",
    "        output_uint8 = sess.run(output_uint8)\n",
    "        # output_uint8 = sess.run(add_result)\n",
    "\n",
    "    return output_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_real(input_data):\n",
    "    ################## stem conv ##################\n",
    "    print('stem conv')\n",
    "    new_data = input_data.reshape(-1, 49, 10, 1)\n",
    "    new_data = tf.convert_to_tensor(new_data, tf.float32, name='input')\n",
    "    new_data =  1.1192268133163452 * (new_data - 221.)\n",
    "    # print(tf.where(new_data < -247.34912109375).eval())\n",
    "    # print(tf.where(new_data > 38.0537109375).eval())\n",
    "\n",
    "    weight = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_weights_quant_FakeQuantWithMinMaxVars.npy')\n",
    "    bias = np.load('test_log/mobilenetv3_quant_eval/weight/MBNetV3-CNN_stem_conv_conv_Conv2D_Fold_bias.npy')\n",
    "    # print(weight.dtype, weight.shape)\n",
    "    # print(bias.dtype, bias.shape)\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight, tf.float32, name='weight')\n",
    "    weight = 0.0009845112217590213 * (weight - 132.)\n",
    "    weight = tf.transpose(weight, perm=[1,2,0,3])   # [filter_height, filter_width, in_channels, out_channels]\n",
    "    low = tf.ones_like(weight) * -0.12866608798503876\n",
    "    high = tf.ones_like(weight) * 0.12139977514743805\n",
    "    weight = tf.where(weight < -0.12866608798503876, low, weight)\n",
    "    weight = tf.where(weight > 0.12139977514743805, high, weight)\n",
    "\n",
    "    bias = tf.convert_to_tensor(bias, tf.float32, name='bias')\n",
    "    bias = 0.0011018913937732577 * bias\n",
    "    # print(weight)\n",
    "    # print(bias)\n",
    "\n",
    "    output = tf.nn.depthwise_conv2d(new_data,  # 张量输入\n",
    "                filter=weight, # 卷积核参数\n",
    "                strides=[1,2,2,1], # 步长参数\n",
    "                padding=\"SAME\", # 卷积方式\n",
    "                data_format=None,  # 数据格式，与步长参数配合，决定移动方式\n",
    "                name='stem_conv') # 名字，用于tensorboard图形显示时使用\n",
    "\n",
    "    output_add = tf.add(output, bias, name='add')\n",
    "    # output_uint8 = tf.cast(output, dtype=tf.uint8)\n",
    "    low = tf.ones_like(output_add) * 0.16148914396762848\n",
    "    high = tf.ones_like(output_add) * 41.17973327636719\n",
    "    output_add = tf.where(output_add < 0.16148914396762848, low, output_add)\n",
    "    output_add = tf.where(output_add > 41.17973327636719, high, output_add)\n",
    "    output_add = output_add / 0.16148914396762848\n",
    "    \n",
    "    output_relu = tf.nn.relu(output_add)\n",
    "    output_uint8 = tf.math.round(output_relu, name='round')\n",
    "    output_uint8 = tf.cast(output_uint8, tf.uint8, name='uint8')\n",
    "    add_2 = tf.identity(output_uint8)   # 给之后的做加法\n",
    "    print()\n",
    "\n",
    "    output_uint8 = sess.run(output_uint8)\n",
    "    return output_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:set_size=4890\n",
      "stem conv\n",
      "\n",
      "[[[[ 1  1  1 ... 34  4  1]\n",
      "   [ 1  6  1 ...  2  1  4]\n",
      "   [ 1  3  1 ...  4  1  6]\n",
      "   [ 1  2  1 ...  5  1  6]\n",
      "   [ 1  3  1 ...  6  1  5]]\n",
      "\n",
      "  [[ 1  5  5 ... 27 10  1]\n",
      "   [ 1  5  1 ...  3  1  5]\n",
      "   [ 1  2  1 ...  3  1  4]\n",
      "   [ 1  1  1 ...  4  1  4]\n",
      "   [ 1  2  1 ...  7  1  5]]\n",
      "\n",
      "  [[ 1  5  1 ... 11  5  1]\n",
      "   [ 1  6  1 ...  5  1  5]\n",
      "   [ 1  2  1 ...  4  1  3]\n",
      "   [ 1  2  1 ...  7  1  4]\n",
      "   [ 1  1  1 ...  5  1  3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1  1  1 ... 14  1  1]\n",
      "   [ 4  8  1 ...  5  3  1]\n",
      "   [ 1  4  5 ...  7  2  6]\n",
      "   [ 1  2  1 ...  3  1  5]\n",
      "   [ 1  1  1 ...  4  1  5]]\n",
      "\n",
      "  [[ 1  1  3 ...  8  1  8]\n",
      "   [ 3  6  1 ... 11  3  2]\n",
      "   [ 1  4  3 ...  5  1  8]\n",
      "   [ 1  4  1 ...  3  1  4]\n",
      "   [ 1  1  1 ...  4  1  6]]\n",
      "\n",
      "  [[ 1  1  1 ...  1  2  8]\n",
      "   [ 4  5  1 ... 14  3  8]\n",
      "   [ 1  3  2 ...  3  1  5]\n",
      "   [ 1  4  1 ...  4  1  5]\n",
      "   [ 1  1  1 ...  5  1  5]]]]\n",
      "[[[[ 0  2  2 ... 34  5  0]\n",
      "   [ 2  7  1 ...  3  0  5]\n",
      "   [ 0  4  1 ...  4  0  6]\n",
      "   [ 0  3  0 ...  6  1  6]\n",
      "   [ 0  3  1 ...  6  1  6]]\n",
      "\n",
      "  [[ 0  5  6 ... 27 10  0]\n",
      "   [ 0  6  0 ...  3  0  5]\n",
      "   [ 0  2  0 ...  3  0  5]\n",
      "   [ 1  1  2 ...  5  2  5]\n",
      "   [ 0  2  1 ...  7  0  6]]\n",
      "\n",
      "  [[ 0  5  0 ... 12  6  0]\n",
      "   [ 0  6  0 ...  5  0  6]\n",
      "   [ 0  3  0 ...  5  0  4]\n",
      "   [ 0  3  0 ...  7  1  5]\n",
      "   [ 0  1  1 ...  5  1  4]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0  0  1 ... 14  0  0]\n",
      "   [ 4  8  0 ...  5  3  2]\n",
      "   [ 1  4  6 ...  7  2  6]\n",
      "   [ 0  2  1 ...  3  2  6]\n",
      "   [ 0  1  1 ...  5  1  5]]\n",
      "\n",
      "  [[ 1  0  3 ...  8  0  8]\n",
      "   [ 4  7  0 ... 11  3  2]\n",
      "   [ 0  5  4 ...  5  1  8]\n",
      "   [ 0  4  1 ...  3  1  4]\n",
      "   [ 0  2  1 ...  5  1  6]]\n",
      "\n",
      "  [[ 1  0  2 ...  0  2  8]\n",
      "   [ 5  5  0 ... 14  3  8]\n",
      "   [ 0  4  3 ...  4  0  6]\n",
      "   [ 1  5  1 ...  4  2  5]\n",
      "   [ 0  2  1 ...  6  1  5]]]]\n",
      "1285 / 2000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "0",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "set_size = audio_processor.set_size('testing')\n",
    "tf.logging.info('set_size=%d', set_size)\n",
    "total_accuracy = 0\n",
    "for i in range(0, set_size, batch_size):\n",
    "    test_fingerprints, test_ground_truth = audio_processor.get_data(\n",
    "        batch_size, i, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "    # print(test_fingerprints.shape)  # (batch_size 490)\n",
    "    # print(test_ground_truth.shape)  # (batch_size, 12)\n",
    "    test_fingerprints = fp32_to_uint8(test_fingerprints)\n",
    "    # output_simulate = manual_int(test_fingerprints)\n",
    "    output_simulate = manual_real(test_fingerprints)\n",
    "    output_real = calc(interpreter, test_fingerprints, test_ground_truth)\n",
    "\n",
    "    # print(output_simulate.shape)\n",
    "    # print(output_real.shape)\n",
    "    print(output_simulate)\n",
    "    print(output_real)\n",
    "    \n",
    "    eq = tf.equal(output_real, output_simulate)\n",
    "    mask = tf.cast(tf.zeros_like(eq), tf.bool)\n",
    "    neq = tf.reduce_sum(tf.cast(tf.equal(eq, mask), tf.int32))\n",
    "    print(sess.run(neq), '/', sess.run(tf.size(eq)))\n",
    "\n",
    "    sys.exit(0)\n",
    "    # batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "\n",
    "\n",
    "'''\n",
    "############################### get all data mean and std_dev ###############################\n",
    "training_fingerprints, training_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'training', sess)\n",
    "validation_fingerprints, validation_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'validation', sess)\n",
    "testing_fingerprints, testing_ground_truth = audio_processor.get_data(\n",
    "    -1, 0, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "mean_, std_dev = data_stats(training_fingerprints, validation_fingerprints, testing_fingerprints)\n",
    "print(mean_, std_dev)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}